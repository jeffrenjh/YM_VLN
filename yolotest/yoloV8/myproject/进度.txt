1.3
写成能够跑ros2bag的yolo目标检测代码

录制ros2bag的指令：
ros2 bag record -o my_d435i_bag /camera/color/image_raw /camera/depth/image_rect_raw /camera/color/camera_info

1.4
ros2在python环境下运行，问题比较多，所以考虑转成录制视频+发送位置请求的方式来离线处理
因为yolo可以接入到ros2中，暂时可以留作后续的优化部分内容

1.5
调研了一番，好像depth的mp4格式有点麻烦？不管了，直接丢给AI处理，思考了一下先试试image的形式
好像也是不太好搞，png深度图，jpg格式rgb这样子
思考一番：如果离线的不太好，就直接实时的，然后把检测到的存到一个文件里，后面再根据标定来转成全局坐标？
感觉可以，具体采集过程：先不要太复杂，从固定的几个位置出发，可以自己决定是否开启坐标转换的功能


1.6
发现了bug【在调用 cv2.cvtColor() 函数时，传入的 color_img 不是有效的 NumPy 数组
（OpenCV 要求的格式），导致颜色空间转换失败。】的真正原因：
conda环境的冲突问题，重装就可以正确采集图像了

运行yolo识别离线图像：
# 如果需要保存结果
python yolo_images.py --input /home/nvidia/huangjie/YM_VLN/test/YM_VLN/yolotest/yoloV8/myproject/data/0106task1 
--output /home/nvidia/huangjie/YM_VLN/test/YM_VLN/yolotest/yoloV8/myproject/data/0106task1_results


1.25
写标定的代码，采取的是物理测量，先把功能跑起来先
imu2euler.py和imutest是我从网上找到的一些代码，能跑起来，但感觉有累积的误差，另外，我暂时没有确定是否结果正确
代码来源：
https://github.com/realsenseai/librealsense/issues/4391
https://blog.csdn.net/qq_41204464/article/details/148840225